## What is it that I will do?

Assess: the situation by understand the impact, the severity and the scale of the incident

Identify: the issues and symptoms that are being experienced in the incident

Act: to mitigate the user symptoms experienced in the incident

Verify: that the mitigation was complete and that the incident is fully resolved


### Jarvis (aka. MDM, Geneva)

[Jarvis Portal](https://aka.ms/jarvis)

[Prerequisites](https://domoreexp.visualstudio.com/DefaultCollection/Teamspace/_git/SkypeTeams-SRE?path=%2Fsre_guides%2Fsetup_permissions.md&version=GBlive&fullScreen=true&_a=contents)


### Why is Jarvis important to me?

Jarvis is the monitoring and dashboard service we use to look at service/client health. There are other sources for dashboards and data analysis, but Jarvis remains the only source of alerts and monitors. The dashboards are also primary sources of information at a glance during a livesite.

#### How to access Jarvis

Jarvis can only be accessed via the web. Even on hotspot connections it may be too slow to load the graphics intensive portal page so please be aware if you are on call.
[https://aka.ms/jarvis](https://aka.ms/jarvis)

#### What Jarvis provides

Jarvis provides a couple of key data visualizations for an active incident:

![alt-text](https://domoreexp.visualstudio.com/DefaultCollection/11ac29bc-5a99-400b-b225-01839ab0c9df/_api/_versioncontrol/itemContent?repositoryId=e7e35b30-7782-4e79-a2fd-9f9bf23e1c0e&path=%2Fsre_guides%2Fimages%2Ftimeslice.png&version=GBlive&contentOnly=true&__v=5  "Jarvis ICM metadata")
1. A timeslice of the metadata that triggered a ticket (Can be found in the description of the ICM ticket)
    - In a Jarvis generated alert the ticket will contain a table of information or metadata 
    - Key data points are:
        - the threshold for the alert to fire
        - the value that triggered the alert
        - the scenarios or API that was associated with the alert
        - success/failure values (can indicate if it is low traffic)
        - the environment which is usually the data center the alert is associated with

![alt-text](https://domoreexp.visualstudio.com/DefaultCollection/11ac29bc-5a99-400b-b225-01839ab0c9df/_api/_versioncontrol/itemContent?repositoryId=e7e35b30-7782-4e79-a2fd-9f9bf23e1c0e&path=%2Fsre_guides%2Fimages%2Fhealth+monitors.png&version=GBlive&contentOnly=true&__v=5  "Health monitor page")
2. A health graph showing the progress of the measure leading to the ticket to fire (show before and after)
    - These are auto generated by each monitor for each dimension combination that returns non-zero values
    - These are helpful indicators of symptom mitigation, but be aware that 0 errors might just mean the users stopped trying
    - These are linked to in the ICM for each incident
    - The timeline can be controlled as such to be able to see before the incident and the values post incident (just remember to update the timeslice)
    - It is often helpful to look at other related charts to see what is happening in different environments or scenarios

![alt-text](https://domoreexp.visualstudio.com/DefaultCollection/11ac29bc-5a99-400b-b225-01839ab0c9df/_api/_versioncontrol/itemContent?repositoryId=e7e35b30-7782-4e79-a2fd-9f9bf23e1c0e&path=%2Fsre_guides%2Fimages%2Fdashboard-example.png&version=GBlive&contentOnly=true&__v=5 "Dashboards")
3. A set of health dashboards that are manually created and provide handy graphs for service data
    - These are manually created graphs and charts for the varying services
    - These are super helpful to understand anomalies related to the incident that might not be alertable
    - These are good for corollating the alert with some service outage or service failure

**Measure**: defn: the values over time that we use to determine thresholds for alerting

Types:
- raw request counts (integers)
- availability percentage (percent)
- request rates [request/time] (double)
- latency (percentile)
- resource utilization (CPU, memory, disk)
- queue lengths
- cache hit rates
- telemetry measures


Examples:
- availability: the percentage of successful requests / total number of requests in a time interval (usually a 1min)
- number of errors: the raw counts of requests with error, it is up to determination what is an error
- request rates: the rate of a subset of requests / total number of requests in a time interval (usually a min)
- queue length: the raw count of the queue length, this is good to identify if there is a processing error

**Monitor**: defn: A monitor or a watchdog is a rule that is calculated over a series of measures. It has a fixed time or sequence interval that it is aware of and that moves along with time. A monitor in Jarvis is configured in the "Manage" tab / "Monitors" subtab. For each monitor, you state the values which the monitor will track against, the time interval and the thresholds to alert on.

In Jarvis terms:
- Each set of monitors is tracked by: [namespace, service, metric]. For example: [SkypeSpaces, SkypeSpacesWusProd, Metric/Api/Response] are the monitors that utilize the api response metric
- The namespace we use will always be "SkypeSpaces"
- The service will differ based on the services we are monitoring, for example: SkypeSpacesWusProd is for Middletier Service
- The metric is collection of measurements that are sent in by the clients. The product code has implemented these set of measurements to provide data back to Jarvis. For example, Metric/Api/Response is a set of events for each middletier api response, it has several dimensions like scenario, response code that are used by monitors. So for each service there can be many different metrics.


Important Concepts:

**Title**: This is the name given to the monitor, it is unique and we try to make them indicate the area they cover, the scenario and possibly the threshold category.

1. Scenario Availability
    - These are scenario based, they each use a calculation of the isPass(MT) or status[Webclient] value for the measures to calculate the availability percentage.
    - The will always list the service it is tracking in [ ], for example, **[MT Availability]** Extensibility - Core
    - The next value will be the feature team that owns those monitors
    - The last value in some part indicates the threshold of the alert used by the monitor
    - They are capable of distinct user counts so that low usage noise can be filtered out
2. SRE - Response Codes
    - These alert on raw error counts of dependencies of the middletier service
    - They only look at response codes from HTTP apis and has heuristically determined thresholds
    - They will often indicate some catastrophic number of errors but won't distinguish between number of tenants or users
    - These can indicate a partner hostname that is failing the underlying service layers

[use this informaiton to trace back to most impacting api]

3. Geneva default titles
    - The format will follow: [Monitor title] dimension used [dimension values]
    - Since this is freeform you'll need to use the dimension information to interpret the dimension values
    - for example: [Status Errors for UsersController] Environment.Scenario.StatusCode [skypeteams.prod.usea-02_UserController_GetUserUnifiedGroupsSettings_429] is unhealthy.
        - Environment: skypeteams.prod.usea-02
        - Scenario: UserController_GetUserUnifiedGroupsSettings
        - StatusCode: 429
        - We can determine that the API GetUserUnifiedGroupSettings is returning 429 out of the Americas east coast

**Status**: This indicates if the monitor definition state.
1. Inactive: Monitor has been defined but is not tracking any measurements
2. Silent: Monitor is tracking measures and has active graphs but the alerts will not fire
3. Active: Monitor is active and will alert

Note:
```
Very important monitors like auth should not be silenced though, because we would prefer you rest the day after instead of letting auth issues go undetected. Silencing monitors is really just a tool to make your sleep and on-call life a better experience.
```

**Dashboard**: These are graphs and charts of the metric measurements Jarvis has consumed

[https://jarvis-west.dc.ad.msft.net/#/dashboard](https://jarvis-west.dc.ad.msft.net/#/dashboard)

Use this to look at the live health after you have started the response or mitigation.  Assess piece


**MT and WebClient Traffic and Availability**

[https://aka.ms/st-sre/trafficdashboard](https://aka.ms/st-sre/trafficdashboard)

How to read the charts: These charts show the request rate and the availability rate for webclient scenarios and middle tier scenarios. As these are the critical two components of the app you are able to look at these to understand if there is some major overall incident happening.

You can determine if:
```
- it is something wrong with webclient or middletier by comparing the availabilities
- there is an outage preventing requests from going to webclient or middletier
- the outage is localized to a particular region
- there are traffic spikes that could be causing latency or throttling
- there are traffic drops indicating telemetry failure
```

Hard to determine if:
```
- which partner service is affected
- blind to other supporting services like firehose, email service, etc.
- blind to individual user outages because it takes all traffic into account it can get minimized
- availability is suceptiable to being weighted more heavily for the most frequently used apis
- a traffic drop is due to the work hours, this should be considered before determining a drop in traffic
``` 

**Webclient Scenario Traffic and Availability Templates**

[https://aka.ms/st-sre/trafficdashboard/scenarios](https://aka.ms/st-sre/trafficdashboard/scenarios)

How to read the charts: These charts cover webclient scenario availability, but the power of these dashboards are that you can filter these down to individual scenarios and you can dive into the history of that scenario. This is important for scenarios that are low traffic and would therefore not be weighted heavily on the more generic availability graphs.

You can determine if:
```
- the historical availability broken down by dialtone or all categories
- the historical availability of any webclient scenario
- the unique users (UU) for any scenario
- the scenario is showing latency issues with the 90th percentile for any given scenario
```

Hard to determine if:
```
- any other clients are not included
- what time of errors are causing the availability drop (timeout vs error vs failure)
- how many tenants are affected
- the impact is regional (doesn't often apply to most webcient scenarios but some scenarios do have external service dependencies)
```

How to select individual scenarios:

1. Navigate to the [scenarios dashboard](https://aka.ms/st-sre/trafficdashboard/scenarios)
2. On the right top buttons there is a filter button (see diagram)
3. Click the "Add +" icon
4. The dimension: name
5. You have a sorted drop down list to add scenarios
6. Click "Apply"

![alt-text](https://domoreexp.visualstudio.com/DefaultCollection/11ac29bc-5a99-400b-b225-01839ab0c9df/_api/_versioncontrol/itemContent?repositoryId=e7e35b30-7782-4e79-a2fd-9f9bf23e1c0e&path=%2Ftsg%2Fimages%2Fscenario_template_selection.png&version=GBlive&contentOnly=true&__v=5  "Selecting a specific scenario") 

**MT HTTP Partner counters**

[https://aka.ms/st-sre/trafficdashboard/partner](https://aka.ms/st-sre/trafficdashboard/partner)

How to read the charts: These charts look at the error counts per hostname. With these counts it can be easier to determine if a particular partner service or dependency is experiencing an outage or issue. The charts will show the steady state of the service api, for example some APIs will have a steady state of 401 or 404 and that should be accounted for before assuming it is a problem. Overall the error rates for nominal are quite low so a jump in error counts can indicate a problem. With these charts you must understand the core HTTP codes that APIs can return.  All the hostnames are from the MT outgoing so there are many calls that are done on the client to partners that are not monitored here.

You can determine if:
```
- there is a service outage with a particular service, the middle charts are divided by major partners (Sharepoint, AAD, Settings Store, EXO, etc) (500 codes)
- there is some kind of throttling happening based onf 429 codes
- there is some jump in requests to a partner (200 codes)
- there is some jump in the nominal error rate to a partner
- there is a jump if user failures to a partner (4xx codes)
- there is problems with auth to a partner (401/403 codes)
```
Hard to determine if:
```
- somethign that affects only a few tenants or users
- something is a bad call pattern vs an outage, a bad release can cause issues similar to a service outage
- other services are bad as it only monitors middletier outgoing calls
```

Note:
```
Throughout your week of on-call you will be FYI on various product code releases. We all wish these were smooth but our monitors are the last line of defense so it will sometime detect issues not found in testing. So be expected to mention releases if tickets come up at about the same time. Often there is a common correlation and it gives us an easy mitigation of rollback.
```


